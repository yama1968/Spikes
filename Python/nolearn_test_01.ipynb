{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannick/bin/anaconda3/envs/py35/lib/python3.5/site-packages/Theano-0.8.0.dev0-py3.5.egg/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import lasagne\n",
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PY2 = sys.version_info[0] == 2\n",
    "\n",
    "if PY2:\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "    def pickle_load(f, encoding):\n",
    "        return pickle.load(f)\n",
    "else:\n",
    "    from urllib.request import urlretrieve\n",
    "\n",
    "    def pickle_load(f, encoding):\n",
    "        return pickle.load(f, encoding=encoding)\n",
    "\n",
    "DATA_URL = 'http://deeplearning.net/data/mnist/mnist.pkl.gz'\n",
    "DATA_FILENAME = 'mnist.pkl.gz'\n",
    "\n",
    "\n",
    "def _load_data(url=DATA_URL, filename=DATA_FILENAME):\n",
    "    \"\"\"Load data from `url` and store the result in `filename`.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"Downloading MNIST dataset\")\n",
    "        urlretrieve(url, filename)\n",
    "\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        return pickle_load(f, encoding='latin-1')\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Get data with labels, split into training, validation and test set.\"\"\"\n",
    "    data = _load_data()\n",
    "    X_train, y_train = data[0]\n",
    "    X_valid, y_valid = data[1]\n",
    "    X_test, y_test = data[2]\n",
    "    y_train = numpy.asarray(y_train, dtype=numpy.int32)\n",
    "    y_valid = numpy.asarray(y_valid, dtype=numpy.int32)\n",
    "    y_test = numpy.asarray(y_test, dtype=numpy.int32)\n",
    "\n",
    "    return dict(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_valid=X_valid,\n",
    "        y_valid=y_valid,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        num_examples_train=X_train.shape[0],\n",
    "        num_examples_valid=X_valid.shape[0],\n",
    "        num_examples_test=X_test.shape[0],\n",
    "        input_dim=X_train.shape[1],\n",
    "        output_dim=10,\n",
    "    )\n",
    "\n",
    "\n",
    "def nn_example(data, max_epochs=60):\n",
    "    net1 = NeuralNet(\n",
    "        layers=[('input', layers.InputLayer),\n",
    "                ('hidden1', layers.DenseLayer),\n",
    "                ('hidden2', layers.DenseLayer),\n",
    "                ('output', layers.DenseLayer),\n",
    "                ],\n",
    "        # layer parameters:\n",
    "        input_shape=(None, 28*28),\n",
    "        hidden1_num_units=800,  # number of units in 'hidden' layer\n",
    "        hidden2_num_units=800,\n",
    "        output_nonlinearity=lasagne.nonlinearities.softmax,\n",
    "        output_num_units=10,  # 10 target values for the digits 0, 1, 2, ..., 9\n",
    "\n",
    "        # optimization method:\n",
    "        update=nesterov_momentum,\n",
    "        update_learning_rate=0.01,\n",
    "        update_momentum=0.9,\n",
    "\n",
    "        max_epochs=max_epochs,\n",
    "        verbose=2,\n",
    "        )\n",
    "\n",
    "    # Train the network\n",
    "    net1.fit(data['X_train'], data['y_train'])\n",
    "\n",
    "    # Try the network on new data\n",
    "    print(\"Feature vector (100-110): %s\" % data['X_test'][0][100:110])\n",
    "    print(\"Label: %s\" % str(data['y_test'][0]))\n",
    "    print(\"Predicted: %s\" % str(net1.predict([data['X_test'][0]])))\n",
    "\n",
    "\n",
    "def main(max_epochs=60):\n",
    "    data = load_data()\n",
    "    print(\"Got %i testing datasets.\" % len(data['X_train']))\n",
    "    nn_example(data, max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raise\n",
      "ignore\n",
      "Got 50000 testing datasets.\n",
      "# Neural Network with 1276810 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       784\n",
      "  1  hidden1     800\n",
      "  2  hidden2     800\n",
      "  3  output       10\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -----\n",
      "      1       \u001b[36m0.50401\u001b[0m       \u001b[32m0.26256\u001b[0m      1.91961      0.92731  9.62s\n",
      "      2       \u001b[36m0.23548\u001b[0m       \u001b[32m0.19906\u001b[0m      1.18293      0.94234  10.42s\n",
      "      3       \u001b[36m0.17738\u001b[0m       \u001b[32m0.16377\u001b[0m      1.08311      0.95213  11.17s\n",
      "      4       \u001b[36m0.14049\u001b[0m       \u001b[32m0.14176\u001b[0m      0.99109      0.95836  10.92s\n",
      "      5       \u001b[36m0.11460\u001b[0m       \u001b[32m0.12662\u001b[0m      0.90506      0.96330  9.57s\n",
      "      6       \u001b[36m0.09520\u001b[0m       \u001b[32m0.11593\u001b[0m      0.82120      0.96538  10.11s\n",
      "      7       \u001b[36m0.08015\u001b[0m       \u001b[32m0.10800\u001b[0m      0.74210      0.96805  10.14s\n",
      "      8       \u001b[36m0.06812\u001b[0m       \u001b[32m0.10208\u001b[0m      0.66737      0.96864  9.56s\n",
      "      9       \u001b[36m0.05825\u001b[0m       \u001b[32m0.09760\u001b[0m      0.59678      0.96973  7.80s\n",
      "     10       \u001b[36m0.05006\u001b[0m       \u001b[32m0.09429\u001b[0m      0.53093      0.97062  9.18s\n",
      "     11       \u001b[36m0.04319\u001b[0m       \u001b[32m0.09193\u001b[0m      0.46987      0.97111  9.31s\n",
      "     12       \u001b[36m0.03739\u001b[0m       \u001b[32m0.09012\u001b[0m      0.41492      0.97181  10.58s\n",
      "     13       \u001b[36m0.03249\u001b[0m       \u001b[32m0.08868\u001b[0m      0.36642      0.97220  10.03s\n",
      "     14       \u001b[36m0.02827\u001b[0m       \u001b[32m0.08767\u001b[0m      0.32250      0.97220  12.39s\n",
      "     15       \u001b[36m0.02467\u001b[0m       \u001b[32m0.08705\u001b[0m      0.28339      0.97299  11.24s\n",
      "     16       \u001b[36m0.02162\u001b[0m       \u001b[32m0.08682\u001b[0m      0.24897      0.97319  11.08s\n",
      "     17       \u001b[36m0.01902\u001b[0m       \u001b[32m0.08674\u001b[0m      0.21925      0.97349  11.24s\n",
      "     18       \u001b[36m0.01681\u001b[0m       \u001b[32m0.08668\u001b[0m      0.19393      0.97359  11.53s\n",
      "     19       \u001b[36m0.01492\u001b[0m       \u001b[32m0.08645\u001b[0m      0.17258      0.97448  12.40s\n",
      "     20       \u001b[36m0.01329\u001b[0m       \u001b[32m0.08585\u001b[0m      0.15478      0.97467  12.65s\n",
      "     21       \u001b[36m0.01185\u001b[0m       \u001b[32m0.08514\u001b[0m      0.13913      0.97477  12.55s\n",
      "     22       \u001b[36m0.01060\u001b[0m       \u001b[32m0.08458\u001b[0m      0.12534      0.97507  10.36s\n",
      "     23       \u001b[36m0.00951\u001b[0m       \u001b[32m0.08393\u001b[0m      0.11333      0.97556  9.98s\n",
      "     24       \u001b[36m0.00857\u001b[0m       \u001b[32m0.08345\u001b[0m      0.10269      0.97586  9.04s\n",
      "     25       \u001b[36m0.00773\u001b[0m       \u001b[32m0.08310\u001b[0m      0.09302      0.97606  9.03s\n",
      "     26       \u001b[36m0.00700\u001b[0m       \u001b[32m0.08289\u001b[0m      0.08443      0.97626  9.78s\n",
      "     27       \u001b[36m0.00635\u001b[0m       \u001b[32m0.08274\u001b[0m      0.07679      0.97655  8.88s\n",
      "     28       \u001b[36m0.00578\u001b[0m       \u001b[32m0.08259\u001b[0m      0.06999      0.97645  9.10s\n",
      "     29       \u001b[36m0.00529\u001b[0m       \u001b[32m0.08256\u001b[0m      0.06403      0.97645  9.15s\n",
      "     30       \u001b[36m0.00485\u001b[0m       0.08257      0.05878      0.97685  11.23s\n",
      "     31       \u001b[36m0.00447\u001b[0m       \u001b[32m0.08255\u001b[0m      0.05420      0.97685  10.17s\n",
      "     32       \u001b[36m0.00414\u001b[0m       0.08258      0.05016      0.97695  9.46s\n",
      "     33       \u001b[36m0.00385\u001b[0m       0.08261      0.04658      0.97715  11.17s\n",
      "     34       \u001b[36m0.00359\u001b[0m       0.08273      0.04336      0.97734  9.09s\n",
      "     35       \u001b[36m0.00335\u001b[0m       0.08285      0.04047      0.97774  9.86s\n",
      "     36       \u001b[36m0.00315\u001b[0m       0.08301      0.03790      0.97774  9.52s\n",
      "Feature vector (100-110): [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Label: 7\n",
      "Predicted: [7]\n"
     ]
    }
   ],
   "source": [
    "print(theano.config.compute_test_value)\n",
    "theano.config.compute_test_value = 'ignore'\n",
    "print(theano.config.compute_test_value)\n",
    "\n",
    "main(max_epochs=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
