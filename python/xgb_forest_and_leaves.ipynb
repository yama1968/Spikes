{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = datasets.fetch_covtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, X_test, y, y_test = train_test_split(\n",
    "    forest.data, forest.target == 2, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintenant un peu de xgboost!\n",
    "\n",
    "### D'abord un tout petit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def logit(p):\n",
    "    return np.log(p) - np.log(1 - p)\n",
    "\n",
    "def inv_logit(p):\n",
    "    return np.exp(p) / (1 + np.exp(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "dtest  = xgb.DMatrix(X_test, label=y_test)\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "param = {\n",
    "    'bst:max_depth': 5, \n",
    "    'bst:eta': 0.5, \n",
    "    'silent': True, \n",
    "    'verbose': 0,\n",
    "    'objective': 'binary:logistic',\n",
    "    'nthread': 4,\n",
    "    'eval_metric': 'auc',\n",
    "    # 'tree_method': 'gpu_hist'\n",
    "}\n",
    "\n",
    "num_round = 4\n",
    "bst = xgb.train(param,\n",
    "                dtrain,\n",
    "                num_round,\n",
    "                evallist,\n",
    "                verbose_eval    = 40\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0, [0,25,35,15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = bst.predict(dtest, pred_leaf=True)\n",
    "z = bst.predict(dtest)\n",
    "leaves[:3], z[:3], inv_logit(0.388039+ 0.354413)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves[400], y[400], X_test[400, [0,7,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.dump_model(\"bst_2_trees\")\n",
    "! cat bst_2_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "dtest  = xgb.DMatrix(X_test, label=y_test)\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "param = {\n",
    "    'bst:max_depth': 5, \n",
    "    'bst:eta': 0.5, \n",
    "    'silent': True, \n",
    "    'verbose': 0,\n",
    "    'objective': 'binary:logistic',\n",
    "    'nthread': 4,\n",
    "    'eval_metric': 'auc',\n",
    "    'tree_method': 'gpu_hist'\n",
    "}\n",
    "\n",
    "num_round = 601\n",
    "bst = xgb.train(param,\n",
    "                dtrain,\n",
    "                num_round,\n",
    "                evallist,\n",
    "                verbose_eval    = 40\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "dtest  = xgb.DMatrix(X_test, label=y_test)\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "param = {\n",
    "    'bst:max_depth': 5, \n",
    "    'bst:eta': 0.5, \n",
    "    'silent': True, \n",
    "    'verbose': 0,\n",
    "    'objective': 'binary:logistic',\n",
    "    'nthread': 4,\n",
    "    'eval_metric': 'auc',\n",
    "    'tree_method': 'hist'\n",
    "}\n",
    "\n",
    "num_round = 601\n",
    "bst = xgb.train(param,\n",
    "                dtrain,\n",
    "                num_round,\n",
    "                evallist,\n",
    "                verbose_eval    = 40\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "param = {\n",
    "    'bst:max_depth': 7, \n",
    "    'bst:eta': 0.15,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'subsample': 0.6,\n",
    "    'silent': True, \n",
    "    'verbose': 0,\n",
    "    'objective': 'binary:logistic',\n",
    "    'nthread': 4,\n",
    "    'eval_metric': 'auc',\n",
    "    'tree_method': 'gpu_hist'\n",
    "}\n",
    "\n",
    "num_round = 1001\n",
    "bst = xgb.train(param,\n",
    "                dtrain,\n",
    "                num_round,\n",
    "                evallist,\n",
    "                verbose_eval    = 40\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "param = {\n",
    "    'bst:max_depth': 7, \n",
    "    'bst:eta': 0.15,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'subsample': 0.6,\n",
    "    'silent': True, \n",
    "    'verbose': 0,\n",
    "    'objective': 'binary:logistic',\n",
    "    'nthread': 4,\n",
    "    'eval_metric': 'auc'\n",
    "}\n",
    "\n",
    "num_round = 1001\n",
    "bst = xgb.train(param,\n",
    "                dtrain,\n",
    "                num_round,\n",
    "                evallist,\n",
    "                verbose_eval    = 40\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X, y)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'auc'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.25,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 40\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=3000,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "# predict\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "print('Feature names:', gbm.feature_name())\n",
    "\n",
    "# feature importances\n",
    "print('Feature importances:', list(gbm.feature_importance()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X, y)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'auc'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.25,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    'device': 'gpu'\n",
    "}\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=3000,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5,\n",
    "                verbose_eval=200)\n",
    "\n",
    "# predict\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "print('Feature names:', gbm.feature_name())\n",
    "\n",
    "# feature importances\n",
    "print('Feature importances:', list(gbm.feature_importance()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
