{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Variational auto-encoder for MNIST data.\n",
    "\n",
    "References\n",
    "----------\n",
    "http://edwardlib.org/tutorials/decoder\n",
    "http://edwardlib.org/tutorials/inference-networks\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Bernoulli, Normal\n",
    "from edward.util import Progbar\n",
    "from keras.layers import Dense\n",
    "from observations import mnist\n",
    "from scipy.misc import imsave\n",
    "\n",
    "\n",
    "ed.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generator(array, batch_size):\n",
    "  \"\"\"Generate batch with respect to array's first axis.\"\"\"\n",
    "  start = 0  # pointer to where we are in iteration\n",
    "  while True:\n",
    "    stop = start + batch_size\n",
    "    diff = stop - array.shape[0]\n",
    "    if diff <= 0:\n",
    "      batch = array[start:stop]\n",
    "      start += batch_size\n",
    "    else:\n",
    "      batch = np.concatenate((array[start:], array[:diff]))\n",
    "      start = diff\n",
    "    batch = batch.astype(np.float32) / 255.0  # normalize pixel intensities\n",
    "    batch = np.random.binomial(1, batch)  # binarize images\n",
    "    yield batch\n",
    "\n",
    "    \n",
    "data_dir = \"/tmp/data\"\n",
    "out_dir = \"/tmp/out\"\n",
    "if not os.path.exists(out_dir):\n",
    "  os.makedirs(out_dir)\n",
    "M = 100  # batch size during training\n",
    "d = 2  # latent dimension\n",
    "\n",
    "# DATA. MNIST batches are fed at training time.\n",
    "(x_train, y_train), (x_test, y_test) = mnist(data_dir)\n",
    "x_train_generator = generator(x_train, M)\n",
    "\n",
    "(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<edward.inferences.klqp.KLqp at 0x7f5671dc7fd0>,\n",
       " <tensorflow.python.training.rmsprop.RMSPropOptimizer at 0x7f5671d43208>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL\n",
    "# Define a subgraph of the full model, corresponding to a minibatch of\n",
    "# size M.\n",
    "z = Normal(loc=tf.zeros([M, d]), scale=tf.ones([M, d]))\n",
    "hidden = Dense(256, activation='relu')(z.value())\n",
    "x = Bernoulli(logits=Dense(28 * 28)(hidden))\n",
    "\n",
    "# INFERENCE\n",
    "# Define a subgraph of the variational model, corresponding to a\n",
    "# minibatch of size M.\n",
    "x_ph = tf.placeholder(tf.int32, (None, 28 * 28))\n",
    "hidden = Dense(256, activation='relu')(tf.cast(x_ph, tf.float32))\n",
    "qz = Normal(loc=Dense(d)(hidden),\n",
    "            scale=Dense(d, activation='softplus')(hidden))\n",
    "\n",
    "# Bind p(x, z) and q(z | x) to the same TensorFlow placeholder for x.\n",
    "inference = ed.KLqp({z: qz}, data={x: x_ph})\n",
    "optimizer = tf.train.RMSPropOptimizer(0.01, epsilon=1.0)\n",
    "inference.initialize(optimizer=optimizer)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "inference, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "600/600 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "-log p(x) <= 159.896\n",
      "Epoch: 2\n",
      "600/600 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "-log p(x) <= 159.821\n",
      "Epoch: 3\n",
      "600/600 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "-log p(x) <= 159.698\n",
      "Epoch: 4\n",
      "600/600 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "-log p(x) <= 159.624\n",
      "Epoch: 5\n",
      "600/600 [100%] ██████████████████████████████ Elapsed: 4s\n",
      "-log p(x) <= 159.612\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 5\n",
    "n_iter_per_epoch = x_train.shape[0] // M\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "  print(\"Epoch: {0}\".format(epoch))\n",
    "  avg_loss = 0.0\n",
    "\n",
    "  pbar = Progbar(n_iter_per_epoch)\n",
    "  for t in range(1, n_iter_per_epoch + 1):\n",
    "    pbar.update(t)\n",
    "    x_batch = next(x_train_generator)\n",
    "    info_dict = inference.update(feed_dict={x_ph: x_batch})\n",
    "    avg_loss += info_dict['loss']\n",
    "\n",
    "  # Print a lower bound to the average marginal likelihood for an\n",
    "  # image.\n",
    "  avg_loss = avg_loss / n_iter_per_epoch\n",
    "  avg_loss = avg_loss / M\n",
    "  print(\"-log p(x) <= {:0.3f}\".format(avg_loss))\n",
    "\n",
    "  # Prior predictive check.\n",
    "  images = x.eval()\n",
    "  for m in range(M):\n",
    "    imsave(os.path.join(out_dir, '%d.png') % m, images[m].reshape(28, 28))\n",
    "    imsave(os.path.join(out_dir, '%d-in.png') % m, x_batch[m].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ed.RandomVariable 'Normal_3/' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
