CreditHistory.NoCredit.AllPaid + CreditHistory.ThisBank.AllPaid +
CreditHistory.PaidDuly + SavingsAccountBonds.lt.100 +
OtherInstallmentPlans.Bank,
data=training,
family="binomial")
summary(three)
get.auc(three)
get.auc(mod_fit_two)
four <- glm(Class ~ . + (.) * (.),
data=training,
family="binomial")
summary(four)
get.auc(four)
summary(four)
summary(four)
l=c("a","b","c")
l*l
lapply(l, function(x){ lapply(l, function(y) {paste(x,y,sep="*")})})
sapply(l, function(x){ sapply(l, function(y) {paste(x,y,sep="*")})})
collet(sapply(l, function(x){ sapply(l, function(y) {paste(x,y,sep="*")})}))
(apply(l, function(x){ apply(l, function(y) {paste(x,y,sep="*")})}))
apply
?apply
data("iris")
library(caret)
data("GermanCredit")
dim(GermanCredit)
source('/home2/yannick2/github/Spikes/R/logistic.regression.test01.R', echo=TRUE)
three
summary(three)
get.auc(mod_fit_one)
get.auc(mod_fit_two)
get.auc(three)
summary(mod_fit_two)
library(readr)
library(xgboost)
devtools::install_github('dmlc/xgboost',subdir='R-package')
install.packages("readr")
library(glmnet)
library(dplyr)
library(caret)
library(doMC)
registerDoMC(cores=4)
data(iris)
X <- as.matrix(iris %>% select(-Species))
y <- as.factor(iris$Species)
clf1 <- glmnet(X, y,
family = "multinomial")
plot(clf1)
cv.clf1 <- cv.glmnet(X,y,
family = "multinomial",
parallel = T)
plot(cv.clf1)
clf2 <- train(Species~.,
data = iris,
trControl = trainControl("cv", 7),
method = "glmnet",
tuneLength = 20)
plot(clf2)
predict(clf2, iris, type="prob")
require(h2o)
l=h2o.init()
h2o.shutdown(l)
l=h2o.init(nthreads=-1)
l=h2o.init(nthreads=-1)
h2o.shutdown(l)
?"h2o.init"
l=h2o.init(nthreads=-1,max_mem_size="10G")
h2o.shutdown(l)
l=h2o.init(nthreads=-1,max_mem_size="12G")
h2o.demo(kmeans)
?demo
demo()
demo(h2o.deeplearning())
demo(h2o.naiveBayes())
demo(h2o.glm)
demo(h2o.gbm)
rm(list=ls())
l
l=h2o.init(nthreads=-1,max_mem_size="12G")
require(h2o)
l=h2o.init(nthreads=-1,max_mem_size="12G")
??h2o
h2o.shutdown(l)
install.packages(c("deepnet", "darch"))
inputs <- matrix(c(0,0,0,1,1,0,1,1),ncol=2,byrow=TRUE)
outputs <- matrix(c(0,1,1,0),nrow=4)
darch <- newDArch(c(2,4,1),batchSize=2)
require(darch)
darch <- newDArch(c(2,4,1),batchSize=2)
darch <- preTrainDArch(darch,inputs,maxEpoch=1000)
layers <- getLayers(darch)
for(i in length(layers):1){
layers[[i]][[2]] <- sigmoidUnitDerivative
}
setLayers(darch) <- layers
rm(layers)
setFineTuneFunction(darch) <- backpropagation
darch <- fineTuneDArch(darch,inputs,outputs,maxEpoch=1000)
darch <- darch <- getExecuteFunction(darch)(darch,inputs)
outputs <- getExecOutputs(darch)
cat(outputs[[length(outputs)]])
outputs
require(deepnet)
?newDArch
?h2O
install.packages("xkcd")
require(xdkcd)
require(xkcd)
data(mtcars)
p <- ggplot() + geom_point(aes(mpg, wt), data=mtcars) + theme_xkcd()
vignette("xkcd-intro")
library(extrafont)
download.file("http://simonsoftware.se/other/xkcd.ttf",
dest="xkcd.ttf", mode="wb")
system("mkdir ~/.fonts")
system("cp xkcd.ttf ~/.fonts")
font_import(pattern = "[X/x]kcd", prompt=FALSE)
fonts()
fonttable()
if(.Platform$OS.type != "unix") {
## Register fonts for Windows bitmap output
loadfonts(device="win")
} else {
loadfonts()
}
p <- ggplot() + geom_point(aes(mpg, wt), data=mtcars) + theme_xkcd()
p
xrange <- range(mtcars$mpg)
yrange <- range(mtcars$wt)
set.seed(123) # for reproducibility
p <- ggplot() + geom_point(aes(mpg, wt), data=mtcars) + xkcdaxis(xrange,yrange)
p
glm
?glm
require(glmnet)
?cv.glmnet
set.seed(1234)
n <- 20
x <- runif(n, -1, 1)
X <- model.matrix(~x)
beta <- c(2, -1)
mu <- crossprod(t(X), beta)
Y <- rpois(n, exp(mu))
foo.model <- function() {
for (i in 1:n) {
Y[i] ~ dpois(lambda[i])
log(lambda[i]) <- inprod(X[i,], beta[1,])
}
for (j in 1:np) {
beta[1,j] ~ dnorm(0, 0.001)
}
}
dat <- list(Y=Y, X=X, n=n, np=ncol(X))
load.module("foo")
m <- jags.fit(dat, "beta", foo.model)
cl <- makePSOCKcluster(3)
## load glm module
tmp <- clusterEvalQ(cl, library(dclone))
parLoadModule(cl, "foo")
pm <- jags.parfit(cl, dat, "beta", foo.model)
## chains are not identical -- this is good
pm[1:2,]
summary(pm)
rm(list=ls())
library('dclone')
library('rjags')
library('snow')
set.seed(1234)
n <- 20
x <- runif(n, -1, 1)
X <- model.matrix(~x)
beta <- c(2, -1)
mu <- crossprod(t(X), beta)
Y <- rpois(n, exp(mu))
foo.model <- function() {
for (i in 1:n) {
Y[i] ~ dpois(lambda[i])
log(lambda[i]) <- inprod(X[i,], beta[1,])
}
for (j in 1:np) {
beta[1,j] ~ dnorm(0, 0.001)
}
}
dat <- list(Y=Y, X=X, n=n, np=ncol(X))
load.module("foo")
m <- jags.fit(dat, "beta", foo.model)
cl <- makePSOCKcluster(3)
## load glm module
tmp <- clusterEvalQ(cl, library(dclone))
parLoadModule(cl, "foo")
pm <- jags.parfit(cl, dat, "beta", foo.model)
## chains are not identical -- this is good
pm[1:2,]
summary(pm)
library('dclone')
library('rjags')
library('snow')
set.seed(1234)
n <- 20
x <- runif(n, -1, 1)
X <- model.matrix(~x)
beta <- c(2, -1)
mu <- crossprod(t(X), beta)
Y <- rpois(n, exp(mu))
foo.model <- function() {
for (i in 1:n) {
Y[i] ~ dpois(lambda[i])
log(lambda[i]) <- inprod(X[i,], beta[1,])
}
for (j in 1:np) {
beta[1,j] ~ dnorm(0, 0.001)
}
}
dat <- list(Y=Y, X=X, n=n, np=ncol(X))
# load.module("foo")
m <- jags.fit(dat, "beta", foo.model)
cl <- makePSOCKcluster(3)
## load glm module
tmp <- clusterEvalQ(cl, library(dclone))
# parLoadModule(cl, "foo")
pm <- jags.parfit(cl, dat, "beta", foo.model)
## chains are not identical -- this is good
pm[1:2,]
summary(pm)
library('dclone')
library('rjags')
library('snow')
set.seed(1234)
n <- 20
x <- runif(n, -1, 1)
X <- model.matrix(~x)
beta <- c(2, -1)
mu <- crossprod(t(X), beta)
Y <- rpois(n, exp(mu))
foo.model <- function() {
for (i in 1:n) {
Y[i] ~ dpois(lambda[i])
log(lambda[i]) <- inprod(X[i,], beta[1,])
}
for (j in 1:np) {
beta[1,j] ~ dnorm(0, 0.001)
}
}
dat <- list(Y=Y, X=X, n=n, np=ncol(X))
# load.module("foo")
m <- jags.fit(dat, "beta", foo.model)
# cl <- makePSOCKcluster(3)
## load glm module
# tmp <- clusterEvalQ(cl, library(dclone))
# parLoadModule(cl, "foo")
pm <- jags.parfit(4, dat, "beta", foo.model)
## chains are not identical -- this is good
pm[1:2,]
summary(pm)
pm <- dc.parfit(4, dat, "beta", foo.model, flavour="jags", n.chains=4)
## chains are not identical -- this is good
pm[1:2,]
summary(pm)
n.chains=4, n.clones=4)
pm <- dc.parfit(4, dat, "beta", foo.model, flavour="jags",
n.chains=4, n.clones=4)
pm <- dc.parfit(4, dat, "beta", foo.model, flavour="jags",
n.chains=4, n.clones=1)
pm <- dc.parfit(4, dat, "beta", foo.model, flavour="jags",
n.chains=4, n.clones=8)
pm <- jags.parfit(4, dat, "beta", foo.model, flavour="jags",
n.chains=4)
summary(pm)
require("h2o")
require("h2oEnsemble")
require("SuperLearner")
require("cvAUC")
require("parallel")
# preparing the data
dat <- read.csv("train.csv", header=TRUE)
#
# and split it into a train and a test data set. The test dataset provided by the Kaggle challenge does not include output labels so we can not use it to test our machine learning model.
#
# We split it by creating a train index that chooses 4000 line numbers from the data frame. We then subset it into train and test:
train_idx <- sample(1:nrow(dat),4000,replace=FALSE)
train <- dat[train_idx,] # select all these rows
test <- dat[-train_idx,] # select all but these rows
# starting h2o
localH2O = h2o.init(nthreads = 4)
training_frame <- as.h2o(localH2O, train)
validation_frame <- as.h2o(localH2O, test)
y <- "Choice"
x <- setdiff(names(training_frame), y)
family <- "binomial"
training_frame[,c(y)] <- as.factor(training_frame[,c(y)])
#Force Binary classification
validation_frame[,c(y)] <- as.factor(validation_frame[,c(y)])
# check to validate that this guarantees the same 0/1 mapping?
# Specify the base learner library & the metalearner
learner <- c("h2o.glm.wrapper", "h2o.randomForest.wrapper",
"h2o.gbm.wrapper", "h2o.deeplearning.wrapper")
metalearner <- "SL.glm"
system.time (
fit <- h2o.ensemble(x = x, y = y,
training_frame = training_frame,
family = family,
learner = learner,
metalearner = metalearner,
cvControl = list(V = 7, shuffle = TRUE),
parallel = "seq",
seed = 3456)
)
pred <- predict.h2o.ensemble(fit, validation_frame)
labels <- as.data.frame(validation_frame[,c(y)])[,1]
# Ensemble test AUC
AUC(predictions=as.data.frame(pred$pred)[,1], labels=labels)
# 0.870
L <- length(learner)
sapply(seq(L), function(l) AUC(predictions = as.data.frame(pred$basepred)[,l], labels = labels))
# [1] 0.8045044 0.8676948 0.8758433 0.8241745
# now deep!
system.time (
#         deep <- h2o.deeplearning(x = x, y = y,
#                                  training_frame = training_frame,
#                                  activation = "RectifierWithDropout",
#                                  epochs = 500,
#                                  hidden = c(500, 500, 500),
#                                  seed = 1234
#                                  )
deep <- h2o.deeplearning(x = x, y = y,
epochs = 80,
activation = "RectifierWithDropout",
training_frame = training_frame,
hidden = c(50, 50),
seed = 1234)
)
pred.deep <- h2o.predict(deep, validation_frame)
labels.deep <- as.data.frame(validation_frame[,c(y)])[,1]
# Ensemble test AUC
AUC(predictions=as.data.frame(pred.deep$pred)[,1], labels=labels.deep)
# [1] 0.7340168
# [1] 0.7616866
setwd("~/github/Spikes/h2o")
require("h2o")
require("h2oEnsemble")
require("SuperLearner")
require("cvAUC")
require("parallel")
# preparing the data
dat <- read.csv("train.csv", header=TRUE)
#
# and split it into a train and a test data set. The test dataset provided by the Kaggle challenge does not include output labels so we can not use it to test our machine learning model.
#
# We split it by creating a train index that chooses 4000 line numbers from the data frame. We then subset it into train and test:
train_idx <- sample(1:nrow(dat),4000,replace=FALSE)
train <- dat[train_idx,] # select all these rows
test <- dat[-train_idx,] # select all but these rows
# starting h2o
localH2O = h2o.init(nthreads = 4)
training_frame <- as.h2o(localH2O, train)
validation_frame <- as.h2o(localH2O, test)
y <- "Choice"
x <- setdiff(names(training_frame), y)
family <- "binomial"
training_frame[,c(y)] <- as.factor(training_frame[,c(y)])
#Force Binary classification
validation_frame[,c(y)] <- as.factor(validation_frame[,c(y)])
# check to validate that this guarantees the same 0/1 mapping?
# Specify the base learner library & the metalearner
learner <- c("h2o.glm.wrapper", "h2o.randomForest.wrapper",
"h2o.gbm.wrapper", "h2o.deeplearning.wrapper")
metalearner <- "SL.glm"
system.time (
fit <- h2o.ensemble(x = x, y = y,
training_frame = training_frame,
family = family,
learner = learner,
metalearner = metalearner,
cvControl = list(V = 7, shuffle = TRUE),
parallel = "seq",
seed = 3456)
)
pred <- predict.h2o.ensemble(fit, validation_frame)
labels <- as.data.frame(validation_frame[,c(y)])[,1]
# Ensemble test AUC
AUC(predictions=as.data.frame(pred$pred)[,1], labels=labels)
# 0.870
L <- length(learner)
sapply(seq(L), function(l) AUC(predictions = as.data.frame(pred$basepred)[,l], labels = labels))
# [1] 0.8045044 0.8676948 0.8758433 0.8241745
# now deep!
system.time (
#         deep <- h2o.deeplearning(x = x, y = y,
#                                  training_frame = training_frame,
#                                  activation = "RectifierWithDropout",
#                                  epochs = 500,
#                                  hidden = c(500, 500, 500),
#                                  seed = 1234
#                                  )
deep <- h2o.deeplearning(x = x, y = y,
epochs = 80,
activation = "RectifierWithDropout",
training_frame = training_frame,
hidden = c(50, 50),
seed = 1234)
)
pred.deep <- h2o.predict(deep, validation_frame)
labels.deep <- as.data.frame(validation_frame[,c(y)])[,1]
# Ensemble test AUC
AUC(predictions=as.data.frame(pred.deep$pred)[,1], labels=labels.deep)
# [1] 0.7340168
# [1] 0.7616866
system.time (
fit <- h2o.ensemble(x = x, y = y,
training_frame = training_frame,
family = family,
learner = learner,
metalearner = metalearner,
cvControl = list(V = 7, shuffle = TRUE),
parallel = "seq",
seed = 1234)
)
pred <- predict.h2o.ensemble(fit, validation_frame)
labels <- as.data.frame(validation_frame[,c(y)])[,1]
# Ensemble test AUC
AUC(predictions=as.data.frame(pred$pred)[,1], labels=labels)
# 0.870
L <- length(learner)
sapply(seq(L), function(l) AUC(predictions = as.data.frame(pred$basepred)[,l], labels = labels))
# [1] 0.8045044 0.8676948 0.8758433 0.8241745
g <- h2o.gbm(x, y,
ntrees=100, max_depth=3)
pred.g <- h2o.pred(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
g <- h2o.gbm(x, y,
training_frame=training_frame,
ntrees=100, max_depth=3)
pred.g <- h2o.pred(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
pred.g <- h2o.predict(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
g <- h2o.gbm(x, y,
training_frame=training_frame,
ntrees=300, max_depth=3)
pred.g <- h2o.predict(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
g <- h2o.gbm(x, y,
training_frame=training_frame,
ntrees=300, max_depth=5)
pred.g <- h2o.predict(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
g <- h2o.gbm(x, y,
training_frame=training_frame,
ntrees=300, max_depth=2)
pred.g <- h2o.predict(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
g <- h2o.gbm(x, y,
training_frame=training_frame,
ntrees=300, max_depth=4)
pred.g <- h2o.predict(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
g <- h2o.gbm(x, y,
training_frame=training_frame,
ntrees=300, max_depth=3, learn_rate = 0.3)
pred.g <- h2o.predict(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
g <- h2o.gbm(x, y,
training_frame=training_frame,
ntrees=300, max_depth=3, learn_rate = 0.6)
pred.g <- h2o.predict(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
g <- h2o.gbm(x, y,
training_frame=training_frame,
ntrees=300, max_depth=3, learn_rate = 0.8)
pred.g <- h2o.predict(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
g <- h2o.gbm(x, y,
training_frame=training_frame,
ntrees=300, max_depth=3, learn_rate = 0.1)
pred.g <- h2o.predict(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
?h2o.gbm
g <- h2o.gbm(x, y,
training_frame=training_frame,
ntrees=30, max_depth=3, learn_rate = 0.1)
pred.g <- h2o.predict(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
g <- h2o.gbm(x, y,
training_frame=training_frame,
ntrees=500, max_depth=3, learn_rate = 0.1)
pred.g <- h2o.predict(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
g <- h2o.gbm(x, y,
training_frame=training_frame,
ntrees=200, max_depth=3, learn_rate = 0.1)
pred.g <- h2o.predict(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
g <- h2o.gbm(x, y,
nfolds=7,
ntrees=200, max_depth=3, learn_rate = 0.1)
pred.g <- h2o.predict(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
g <- h2o.gbm(x, y,
training_frame=training_frame,
nfolds=7,
ntrees=200, max_depth=3, learn_rate = 0.1)
pred.g <- h2o.predict(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
g <- h2o.gbm(x, y,
training_frame=training_frame,
nfolds=7,
ntrees=200, max_depth=3, learn_rate = 0.1,
model_id="one_gbm")
pred.g <- h2o.predict(g, validation_frame)
AUC(predictions=as.data.frame(pred.g$pred)[,1], labels=labels)
h2o.shutdown(localH2O)
